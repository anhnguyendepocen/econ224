---
title: "Lab #7 - More on Regression in R"
author: "Econ 224"
date: "September 18th, 2018"
---

<!-- knitr global options -->
```{r, include = FALSE}
knitr::opts_chunk$set(comment=NA, fig.width=4.5, fig.height=3.5, fig.align = 'center')
```




# Robust Standard Errors
Your reading assignment from Chapter 3 of ISL briefly discussed two ways that the standard regression inference formulas built into R can go wrong: (1) non-constant error variance, and (2) correlation between regression errors. 
Today we'll briefly look at the first of these problems and how to correct for it.

Consider the simple linear regression $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$.
If the variance of $\epsilon_i$ is unrelated to the value of the predictor $x_i$, we say that the regression errors are *homoskedastic*.
This is just a fancy Greek work for *constant variance*.
If instead, the variance of $\epsilon_i$ depends on the value of $x_i$, we say that the regression errors are *heteroskedastic*.
This is just a fancy Greek word for *non-constant variance*.
Heteroskedasticity does not invalidate our least squares estimates of $\beta_0$ and $\beta_1$, but it does invalidate the formulas used by `lm` to calculate standard errors and p-values.

Let's look at a simple simulation example:
```{r,message = FALSE}
set.seed(4321)
n <- 100
x <- runif(n)
e1 <- rnorm(n, mean = 0, sd = sqrt(2 * x))
e2 <- rnorm(n, mean = 0, sd = 1)
intercept <- 0.2
slope <- 0.9
y1 <- intercept + slope * x + e1
y2 <- intercept + slope * x + e2
library(tidyverse)
mydat <- tibble(x, y1, y2)
rm(x, y1, y2)
```

# Exercise \#1
*[approx 10 min]*

1. Read through my simulation code and make sure you understand what each step is going. What is the distribution of the errors? What is the distribution of `x`? In the simulation design, is there a relationship between `x` and `y1`? What about `y2`?
2. For each of the two simulated outcome variables `y1` and `y2`, plot the outcome against `x` along with the linear regression line.
3. Based on your plots from part 2 and the simulation code, which errors are heteroskedastic: `e1`, `e2`, both, or neither? How can you tell? 

# Solution to Exercise \#1
<!-- ANS_START -->
1. `x` is uniform and the errors are normally distributed. There is indeed a relationship between `x` and `y`: the conditional mean of `y1` given `x` is `0.2 + 0.4 x` and the same is true of `y2`
2. Here is a simple way to make the plots:
```{r}
library(ggplot2)
ggplot(mydat, aes(x, y1)) +
  geom_smooth(method = 'lm') + 
  geom_point()
ggplot(mydat, aes(x, y2)) +
  geom_smooth(method = 'lm') +
  geom_point()
```
3. The errors `e1` are heteroskedastic while the errors `e2` are homoskedastic. We can see this both from plotting the data which "fan out" around the regression line for `y1` and from the simulation code: to generate `e1` we multiplied some normal random draws by the value of `x` so the variance clearly depends on `x`
<!-- ANS_END -->

## Robust Standard Errors using `lm_robust`
Install the package `estimatr`. 
Provides a replacement for `lm` called `lm_robust` that allows us to choose robust standard errors
```{r, message = FALSE}
library(estimatr)
reg1_classical <- lm_robust(y1 ~ x, mydat, se_type = 'stata')
summary(reg1_classical)
reg1_robust <- lm_robust(y1 ~ x, mydat, se_type = 'classical')
summary(reg1_robust)
```
The nice thing about using `lm_robust` is that it plays nicely with `linearHypothesis` for carrying out F-tests.
In an example with only one regressor the F-test is completely superfluous (the F-test statistic is simply the square of the t-test statistic for the slope!) but just to see that it works:
```{r, message = FALSE}
library(car)
summary(lm(y1 ~ x, mydat))$fstatistic
linearHypothesis(reg1_classical, 'x = 0')
linearHypothesis(reg1_robust, 'x = 0')
```

# Exercise \#2
*[approx 5 min]*

Repeat my inference comparison from above for the regression `y2 ~ x` using classical and robust standard errors.
Explain your results.
Do we need to use robust standard errors in this case?
Why or why not?

# Solution to Exercise \#2
We do not in fact need robust standard errors in this case since the errors are homoskedastic.
If we wanted to be on the safe side, we could still use them, but notice that the results are slightly different.
<!-- ANS_START -->
```{r}
reg2_classical <- lm_robust(y2 ~ x, mydat, se_type = 'stata')
summary(reg1_classical)
reg2_robust <- lm_robust(y2 ~ x, mydat, se_type = 'classical')
summary(reg1_robust)
summary(lm(y2 ~ x, mydat))$fstatistic
linearHypothesis(reg2_classical, 'x = 0')
linearHypothesis(reg2_robust, 'x = 0')
```
<!-- ANS_END -->


# Exercise
Go back and do robust standard errors for one of the questions from the previous problem set.


# Publication-quality Tables with `stargazer`
We'll use the package `stargazer` to generate pretty tables of results like the ones you see in journal articles.
Make sure to install this package before proceeding.

```{r, message = FALSE}
library(stargazer)
```

### Simple table of summary statistics
I chose to output my `.Rmd` file to a pdf using LaTeX, so I used the option `type = latex`. 
If you're using `html` you'll need to change this to `type = 'html'`.
If you want to see a "preview" of the table within R studio without compiling, choose `type = 'text'`.
Also notice I'm using the `knitr` option `asis`.
You'll need this to make sure that the `stargazer` table knits correctly.
```{r, results = "asis"}
stargazer(mtcars, type = 'latex')
```

The `stargazer` command provides dozens of options for customizing the appearance of the output it generates. 
Here's a nicer version of the preceding table that uses some of these options:
```{r, results = "asis"}
mylabels <- c('Miles/gallon', 
              'No. of cylinders', 
              'Displacement (cubic inches)', 
              'Horsepower', 
              'Rear axle ratio', 
              'Weight (1000lb)', 
              '1/4 Mile Time', 
              'V/S', 
              'Manual Transmission? (1 = Yes)', 
              'No. forward gears', 
              'No. carburetors')
stargazer(mtcars, 
          type = 'latex', 
          title = 'Summary Statistics: Motor Trend Cars Dataset', 
          digits = 1, 
          header = FALSE,
          covariate.labels = mylabels)
```
We can also customize which summary statistics are reported using the options `summary.stat` and `omit.summary.stat`.
For example, if we only wanted to show the mean, standard deviation, and quartiles of the data, we could use the following:
```{r, results = "asis"}
stargazer(mtcars, 
          type = 'latex', 
          title = 'Summary Statistics: Motor Trend Cars Dataset', 
          digits = 1, 
          header = FALSE,
          covariate.labels = mylabels, 
          summary.stat = c('mean',
                           'sd',
                           'p25',
                           'median',
                           'p75'))
```

# Exercise
Figure out what each of the options `title`, `digits`, `header` and `covariate.labels` does in the preceding code chunk.
Write a one sentence explanation of each.
Try experimenting with different values and or consulting the documentation for `stargazer`.

# Exercise
Make a pretty table of summary statistics from one of the recent problem set questions.


### Regression Output
If you pass a dataframe (or tibble) to `stargazer`, by default it will create a table of summary statistics.
If you instead pass a *regression* object, it will make a regression table.
For example:
Run a bunch of regressions using `mtcars`
```{r, results = 'asis'}
reg1 <- lm(mpg ~ disp, mtcars)
stargazer(reg1, type = 'latex',
          header = FALSE,
          digits = 1,
          title = 'Predicting Fuel Economy from Displacement')
```
Let's run a few more regressions and make a table that summarizes the results of *all* of them:
```{r, results = 'asis'}
reg2 <- lm(mpg ~ wt, mtcars)
reg3 <- lm(mpg ~ disp + wt, mtcars)
stargazer(reg1, reg2, reg3, 
          type = 'latex', 
          digits = 1,
          header = FALSE,
          title = 'Regression Results for Motor Trend Dataset',
          covariate.labels = c('Displacement (cubic inches)', 'Weight (1000lb)'),
          dep.var.caption = 'Miles/gallon',
          notes = c('Data are courtesy of Motor Trend Magazine. Also, R rules!'))
```
Notice how I added a label for the dependent variable and appended a *note* to the regression table.

# Exercise
Make a pretty regression table for one of the problems on the last problem set.
Use robust standard errors.

