---
title: "Lab #13 - Resampling Methods"
author: "Econ 224"
date: "October 23rd, 2018"
---

<!-- knitr global options -->
```{r, include = FALSE}
knitr::opts_chunk$set(comment=NA, fig.width=4.5, fig.height=3.5, fig.align = 'center')
```

## Introduction
In this lab you will work through Section 5.3 of ISL and record your code and results in an RMarkdown document.
I have added section headings below to help you organize your results.
You do not have to submit this lab, so you don't have to type up a detailed description of what you've done.
However, I'd suggest that you write down some notes for your own future reference.
These will be helpful on the problem set.

You do not need to follow the code in ISL exactly: feel free to use your preferred coding style. 
In particular, rather than using the `attach` command suggested by ISL, feel free to instead use `dplyr` or related commands to produce more readable and error-resistant code.
If you choose to do this, you will need to select the rows of a tibble *by position*, i.e.\ by row index.
You can do this using the `slice` function in `dplyr`.

You will need the `ISLR` package for the lab, so please install it if you have not done so already.
Note that this lab uses two different datasets: `Auto` and `Portfolio`.
Both of these are included with `ISLR`.
Make sure to read the documentation for each dataset in the R help files before proceeding.
There are also a few new R functions that you will encounter in this tutorial `poly`, `cv.glm`, and `boot`.
Make sure to read the help files for these functions to make sure that you understand how they work.

## The Validation Set Approach 
Work through section 5.3.1 of ISL and add your code and results below.

<!-- ANS_START -->
```{r, message = FALSE}
library(ISLR)
library(tidyverse)
#-------------------- create training and test samples
set.seed(1)
train_indices <- sample(nrow(Auto), 196)
train <- Auto %>% slice(train_indices)
test <- Auto %>% slice(-train_indices)

#-------------------- fit regressions using training sample
fit1 <- lm(mpg ~ horsepower, train)
fit2 <- lm(mpg ~ poly(horsepower, 2), train)
fit3 <- lm(mpg ~ poly(horsepower, 3), train)

#-------------------- predict test data
predicted1 <- predict(fit1, test)
predicted2 <- predict(fit2, test)
predicted3 <- predict(fit3, test)

#-------------------- compare MSE of predictions
test %>% summarize(MSE1 = mean((mpg - predicted1)^2),
                   MSE2 = mean((mpg - predicted2)^2), 
                   MSE3 = mean((mpg - predicted3)^2))

#-------------------- try re-running with a different seed
```

<!-- ANS_END -->

## Leave-One-Out Cross-Validation 
Work through section 5.3.2 of ISL and add your code and results below.

<!-- ANS_START -->
```{r, message = FALSE}
library(boot)
#--------------------------- Function to automate LOO-CV for polynomial fits
loo_cv_poly <- function(degree) {
  glm_fit <- glm(mpg ~ poly(horsepower, degree), data = Auto)
  cv_error <- cv.glm(Auto, glm_fit)
  cv_error$delta
}
sapply(1:5, loo_cv_poly)
```
<!-- ANS_END -->

## k-Fold Cross-Validation 
Work through section 5.3.3 of ISL and add your code and results below.

<!-- ANS_START -->
```{r}
#--------------------------- Function to automate LOO-CV for polynomial fits
tenfold_cv_poly <- function(degree) {
  glm_fit <- glm(mpg ~ poly(horsepower, degree), data = Auto)
  cv_error <- cv.glm(Auto, glm_fit, K = 10)
  cv_error$delta[1]
}
set.seed(17)
sapply(1:10, tenfold_cv_poly)
```
<!-- ANS_END -->

# The Bootstrap
Work through section 5.3.4 of ISL and add your code and results below.

## Estimating the Accuracy of a Statistic of Interest
<!-- ANS_START -->
```{r}
#--------------------------- Function for use with boot() in portfolio example
get_alpha <- function(dat, indices) {
  X <- dat$X[indices]
  Y <- dat$Y[indices]
  (var(Y) - cov(X,Y)) / (var(X) + var(Y) - 2 * cov(X,Y))
}
set.seed(1)

#--------------------------- Bootstrap sample "by hand"
get_alpha(Portfolio, sample(100, 100, replace = TRUE))
#--------------------------- Bootstrap using boot()
boot(Portfolio, get_alpha, R = 1000)
```
<!-- ANS_END -->

## Estimating the Accuracy of a Linear Regression Model
<!-- ANS_START -->
```{r}
#--------------------------- Function to calculate regression coefs for use with boot()
get_coefs <- function(dat, indices) {
  reg <- lm(mpg ~ horsepower, dat, subset = indices)
  coef(reg)
}
#--------------------------- Bootstrap samples "by hand"
set.seed(1)
get_coefs(Auto, sample(392, 392, replace = TRUE))
get_coefs(Auto, sample(392, 392, replace = TRUE))
#--------------------------- Bootstrap using boot()
boot(Auto, get_coefs, R = 1000)
#--------------------------- Compare to output from lm
summary(lm(mpg ~ horsepower, Auto))
#--------------------------- Repeat the above for a quadratic regression
get_coefs2 <- function(dat, indices) {
  reg <- lm(mpg ~ poly(horsepower, 2), dat, subset = indices)
  coef(reg)
}
set.seed(1)
boot(Auto, get_coefs2, R = 1000)
summary(lm(mpg ~ poly(horsepower, 2), Auto))
```
<!-- ANS_END -->
